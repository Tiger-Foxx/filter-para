# ğŸ¯ğŸ¦Š TIGER-FOX - DUAL MODE IMPLEMENTATION

**Architecture ultra-rapide de filtrage rÃ©seau avec modes sÃ©quentiel et parallÃ¨le**

---

## ğŸ“‹ TABLE DES MATIÃˆRES

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture des deux modes](#architecture-des-deux-modes)
3. [Mode Sequential](#mode-sequential)
4. [Mode Parallel](#mode-parallel)
5. [Partitionnement des rÃ¨gles](#partitionnement-des-rÃ¨gles)
6. [Performance attendue](#performance-attendue)
7. [Commandes de test](#commandes-de-test)
8. [DÃ©tails techniques](#dÃ©tails-techniques)

---

## ğŸ¯ VUE D'ENSEMBLE

### Objectif de la recherche
Prouver que le **parallÃ©lisme multi-core** amÃ©liore les performances de filtrage rÃ©seau L3/L4 par rapport Ã  une approche sÃ©quentielle optimisÃ©e.

### Target performance
- **Baseline (ancien)**: ~700 req/s avec workers pool et TCP reassembly
- **Objectif**: > **2,500 req/s** (battre Suricata/Snort)
- **Attendu Sequential**: 2,000-3,000 req/s
- **Attendu Parallel**: 3,500-6,000 req/s (speed-up 1.5-3x)

### Concept clÃ©
- **Sequential**: 1 thread check NÃ—M rÃ¨gles (M = nombre de workers parallÃ¨les)
- **Parallel**: M workers permanents checkent chacun N rÃ¨gles en parallÃ¨le
- **Partitionnement**: Chaque worker a M fois moins de rÃ¨gles â†’ plus rapide mÃªme pour ACCEPT

---

## ğŸ—ï¸ ARCHITECTURE DES DEUX MODES

### Fichier de rÃ¨gles actuel
- **23 rÃ¨gles** dans `example_rules.json`
- **3 workers** par dÃ©faut en mode parallel
- 10 rÃ¨gles L3 (IP ranges)
- 13 rÃ¨gles L4 (ports TCP/UDP)

### Mode SEQUENTIAL (baseline)
```
NFQUEUE â†’ PacketHandler â†’ ParsePacket (L3/L4 only, zero-copy)
              â†“
         FastSequentialEngine
              â†“
         Hash O(1) lookups
         â€¢ blocked_ips_ (unordered_set)
         â€¢ blocked_tcp_ports_ (unordered_set)
         â€¢ ip_ranges_ (vector, small)
              â†“
         Check 69 rÃ¨gles (23 Ã— 3)
         Temps: 69 Ã— t
              â†“
         ACCEPT/DROP (immediate, inline)
```

**CaractÃ©ristiques:**
- 1 thread unique
- 69 rÃ¨gles (23 Ã— 3 pour Ã©quilibrer vs parallel)
- Hash tables O(1) pour IP/port
- Zero malloc (stack allocation)
- Pas de mutex, pas de threads

### Mode PARALLEL (3 workers permanents)
```
NFQUEUE â†’ PacketHandler â†’ FilterPacket()
              â†“
    [Packet shared via const pointer]
              â†“
    condition_variable.notify_all()
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Worker 1    Worker 2   Worker 3
(rules 0-7) (rules 8-15) (rules 16-22)
    â†“           â†“          â†“
  Check       Check      Check
  8 rÃ¨gles    8 rÃ¨gles   7 rÃ¨gles
  (hash O(1)) (hash O(1)) (hash O(1))
    â†“           â†“          â†“
    [Racing avec atomic CAS]
              â†“
    If DROP found â†’ verdict_found.CAS(falseâ†’true)
              â†“
    All workers finish
              â†“
    condition_variable.notify_one()
              â†“
         Return verdict
         Temps: max(8t, 8t, 7t) = 8t
```

**CaractÃ©ristiques:**
- 3 threads PERMANENTS (crÃ©Ã©s au dÃ©marrage, pas par paquet!)
- Chaque worker: ~8 rÃ¨gles (23 / 3)
- Synchronisation via condition variables (~200ns overhead)
- Racing avec atomic CAS (lock-free, ~10ns)
- Zero-copy packet sharing (const pointer)
- Early exit si DROP trouvÃ©

---

## ğŸ“Š MODE SEQUENTIAL

### ImplÃ©mentation: FastSequentialEngine

**Fichiers:**
- `src/engine/fast_sequential_engine.h`
- `src/engine/fast_sequential_engine.cpp`

### Optimisations

#### 1. Hash Tables O(1)
```cpp
std::unordered_set<uint32_t> blocked_ips_;        // O(1) IP lookup
std::unordered_set<uint16_t> blocked_tcp_ports_;  // O(1) port lookup
std::unordered_set<uint16_t> blocked_udp_ports_;  // O(1) port lookup
std::vector<IPRange> ip_ranges_;                  // O(n) mais n petit (<100)
```

#### 2. Pre-indexation au dÃ©marrage
```cpp
void BuildOptimizedStructures() {
    // Construire hash tables une seule fois
    for (auto& rule : rules) {
        if (rule->type == IP_RANGE) {
            for (auto& range : rule->ip_ranges_) {
                ip_ranges_.push_back(range);
                // Pour petits ranges (/30, /31, /32), extraire IPs individuelles
                ExtractIPsFromRange(range);  // â†’ blocked_ips_
            }
        }
        if (rule->type == PORT) {
            blocked_tcp_ports_.insert(port);
        }
    }
}
```

#### 3. Filtering ultra-rapide
```cpp
FilterResult FilterPacket(const PacketData& packet) {
    // L3: O(1) hash lookup
    if (blocked_ips_.count(src_ip) > 0) return DROP;
    if (blocked_ips_.count(dst_ip) > 0) return DROP;
    
    // L3: O(n) range check (n petit)
    for (auto& range : ip_ranges_) {
        if ((src_ip & range.mask) == range.network) return DROP;
    }
    
    // L4: O(1) hash lookup
    if (blocked_tcp_ports_.count(dst_port) > 0) return DROP;
    
    return ACCEPT;
}
```

### Duplication des rÃ¨gles (pour Ã©quilibrer)

**Dans `tiger_system.cpp`:**
```cpp
if (mode == "sequential") {
    // Dupliquer les rÃ¨gles num_workers fois
    for (size_t copy = 0; copy < 3; ++copy) {
        for (auto& rule : original_rules) {
            auto cloned_rule = rule->Clone();
            cloned_rule->id = rule->id + "_copy" + std::to_string(copy);
            cloned_rule->CompileIPRanges();  // Recompiler masques rÃ©seau
            multiplied_rules[layer].push_back(cloned_rule);
        }
    }
    // Total: 23 Ã— 3 = 69 rÃ¨gles
}
```

---

## âš¡ MODE PARALLEL

### ImplÃ©mentation: UltraParallelEngine

**Fichiers:**
- `src/engine/ultra_parallel_engine.h`
- `src/engine/ultra_parallel_engine.cpp`

### Architecture avec threads permanents

#### 1. Workers permanents crÃ©Ã©s au dÃ©marrage
```cpp
UltraParallelEngine::UltraParallelEngine(...) {
    // Partitionner les 23 rÃ¨gles entre 3 workers
    for (worker_id = 0; worker_id < 3; ++worker_id) {
        // Worker 0: rÃ¨gles 0-7   (8 rÃ¨gles)
        // Worker 1: rÃ¨gles 8-15  (8 rÃ¨gles)
        // Worker 2: rÃ¨gles 16-22 (7 rÃ¨gles)
        
        auto worker_rules = PartitionRules(worker_id);
        workers_[worker_id].engine = new FastSequentialEngine(worker_rules);
        
        // DÃ©marrer thread PERMANENT
        workers_[worker_id].thread = std::thread(WorkerThreadLoop, worker_id);
    }
}
```

#### 2. Thread loop permanent
```cpp
void WorkerThreadLoop(size_t worker_id) {
    while (true) {
        // 1. Attendre notification de nouveau paquet
        std::unique_lock<std::mutex> lock(packet_mutex_);
        packet_ready_cv_.wait(lock, []() {
            return packet_available_ || shutdown_;
        });
        
        if (shutdown_) break;  // ArrÃªt propre
        
        // 2. Traiter le paquet avec MES rÃ¨gles uniquement
        FilterResult result = workers_[worker_id].engine->FilterPacket(*current_packet_);
        
        // 3. Si DROP trouvÃ©, essayer de gagner la course
        if (result.action == DROP) {
            bool expected = false;
            if (verdict_found_.compare_exchange_strong(expected, true)) {
                // ğŸ† J'ai gagnÃ©!
                race_state_.result = result;
            }
        }
        
        // 4. Signaler que j'ai fini
        workers_finished_++;
        if (workers_finished_ == 3) {
            workers_done_cv_.notify_one();  // RÃ©veiller le thread principal
        }
    }
}
```

#### 3. Distribution des paquets
```cpp
FilterResult FilterPacket(const PacketData& packet) {
    // Reset Ã©tat de la course
    race_state_.Reset();
    workers_finished_ = 0;
    
    // Partager paquet (zero-copy via const pointer)
    current_packet_ = &packet;
    packet_available_ = true;
    
    // RÃ©veiller les 3 workers
    packet_ready_cv_.notify_all();
    
    // Attendre que tous aient fini
    std::unique_lock<std::mutex> lock(packet_mutex_);
    workers_done_cv_.wait(lock, []() {
        return workers_finished_ == 3;
    });
    
    // Retourner verdict
    packet_available_ = false;
    if (verdict_found_) {
        return race_state_.result;  // DROP
    }
    return ACCEPT;
}
```

### Synchronisation lock-free

#### Atomic CAS (Compare-And-Swap)
```cpp
struct RaceState {
    std::atomic<bool> verdict_found{false};  // Flag partagÃ©
    std::atomic<int> winner_id{-1};
    FilterResult result;
    std::mutex result_mutex;  // Seulement pour Ã©crire result
};

// Dans worker thread:
bool expected = false;
if (verdict_found.compare_exchange_strong(expected, true)) {
    // âœ… SuccÃ¨s! Je suis le premier
    winner_id = worker_id;
    result = my_result;
} else {
    // âŒ RatÃ©, un autre worker Ã©tait plus rapide
    // â†’ Exit immÃ©diatement
}
```

**Avantages CAS:**
- âœ… Lock-free (pas de mutex pendant check)
- âœ… Ultra-rapide (~10ns, 1 instruction CPU atomique)
- âœ… Pas de contention

---

## ğŸ”§ PARTITIONNEMENT DES RÃˆGLES

### Concept

Avec **23 rÃ¨gles** et **3 workers**:

```
RÃ¨gles originales (0-22):
[0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22]

Partitionnement:
Worker 0: [0-7]       â†’ 8 rÃ¨gles
Worker 1: [8-15]      â†’ 8 rÃ¨gles
Worker 2: [16-22]     â†’ 7 rÃ¨gles
```

### ImplÃ©mentation

```cpp
// Dans ultra_parallel_engine.cpp
std::vector<Rule*> all_rules;
for (auto& [layer, layer_rules] : rules_by_layer_) {
    for (auto& rule : layer_rules) {
        all_rules.push_back(rule.get());
    }
}

size_t rules_per_worker = 23 / 3 = 7;
size_t remainder = 23 % 3 = 2;

for (worker_id = 0; worker_id < 3; ++worker_id) {
    size_t num_rules = rules_per_worker;
    if (worker_id < remainder) num_rules++;  // Distribuer le reste
    
    // Worker 0: 7+1 = 8 rÃ¨gles
    // Worker 1: 7+1 = 8 rÃ¨gles
    // Worker 2: 7+0 = 7 rÃ¨gles
    
    for (i = 0; i < num_rules; ++i) {
        auto cloned = all_rules[start_idx++]->Clone();
        cloned->CompileIPRanges();  // IMPORTANT!
        worker_rules[layer].push_back(cloned);
    }
}
```

### Pourquoi CompileIPRanges()?

**CompileIPRanges()** convertit CIDR notation en masques binaires:

```cpp
void Rule::CompileIPRanges() {
    // "192.168.1.0/24" â†’ {network: 0xC0A80100, mask: 0xFFFFFF00}
    for (auto& cidr : values) {
        size_t slash = cidr.find('/');
        std::string network_str = cidr.substr(0, slash);
        int prefix_len = std::stoi(cidr.substr(slash + 1));
        
        uint32_t network = IPStringToUint32(network_str);
        uint32_t mask = 0xFFFFFFFF << (32 - prefix_len);
        
        ip_ranges_.push_back({network & mask, mask});
    }
}
```

**Sans compilation**, les comparaisons seraient en string â†’ TRÃˆS LENT!  
**Avec compilation**, comparaison binaire ultra-rapide:
```cpp
if ((packet_ip & range.mask) == range.network) return DROP;  // ~2 cycles CPU
```

---

## ğŸ“Š PERFORMANCE ATTENDUE

### Overhead par opÃ©ration

| OpÃ©ration | Temps |
|-----------|-------|
| Hash lookup O(1) | ~10-20ns |
| IP range check (masked & compare) | ~2ns |
| condition_variable.notify_all() | ~200ns |
| Atomic CAS | ~10ns |
| Thread crÃ©ation (ancien) | ~30-50Âµs âŒ |

### Throughput thÃ©orique

#### Sequential (69 rÃ¨gles)
```
Temps par paquet:
- 2 IP checks Ã— 10ns = 20ns (hash lookup)
- 10 IP ranges Ã— 2ns = 20ns (masked compare)
- 2 port checks Ã— 10ns = 20ns (hash lookup)
Total: ~60ns par rÃ¨gle Ã— 69 = 4.14Âµs

Throughput: 1 / 4.14Âµs = 241,000 paquets/s
```

#### Parallel (3 workers Ã— ~8 rÃ¨gles)
```
Temps par paquet:
- Worker check: 8 rÃ¨gles Ã— 60ns = 0.48Âµs (en parallÃ¨le)
- Synchronization overhead: ~0.5Âµs (notify + wait)
Total: max(0.48, 0.48, 0.42) + 0.5 = 0.98Âµs

Throughput: 1 / 0.98Âµs = 1,020,000 paquets/s
```

#### Speed-up
```
Parallel / Sequential = 1,020,000 / 241,000 = 4.2x
```

### Cas DROP vs ACCEPT

#### Paquet DROP (rare ~1-5%)

**Sequential:**
```
Check rÃ¨gles 0-69 â†’ trouve DROP Ã  rÃ¨gle #30
Temps: 30 Ã— 60ns = 1.8Âµs
```

**Parallel:**
```
Worker 0: Check 0-7   â†’ ACCEPT (0.48Âµs)
Worker 1: Check 8-15  â†’ DROP Ã  #12! (0.24Âµs) ğŸ†
Worker 2: Check 16-22 â†’ exit early (0.1Âµs)

Temps: max(0.48, 0.24, 0.1) + 0.5Âµs = 0.98Âµs
```

**Parallel gagne: 1.8Âµs â†’ 0.98Âµs** âœ…

#### Paquet ACCEPT (frÃ©quent ~95-99%)

**Sequential:**
```
Check TOUTES les 69 rÃ¨gles â†’ ACCEPT
Temps: 69 Ã— 60ns = 4.14Âµs
```

**Parallel:**
```
Worker 0: Check 8 rÃ¨gles â†’ ACCEPT (0.48Âµs)
Worker 1: Check 8 rÃ¨gles â†’ ACCEPT (0.48Âµs)
Worker 2: Check 7 rÃ¨gles â†’ ACCEPT (0.42Âµs)

Temps: max(0.48, 0.48, 0.42) + 0.5Âµs = 0.98Âµs
```

**Parallel gagne QUAND MÃŠME: 4.14Âµs â†’ 0.98Âµs** âœ…âœ…âœ…

**â†’ Le partitionnement rend le parallel plus rapide dans TOUS les cas!**

---

## ğŸš€ COMMANDES DE TEST

### 1. Compilation
```bash
cd /home/guest/filter-para
sudo ./build.sh
```

### 2. Configuration iptables (sur filter node)
```bash
# Nettoyer
sudo iptables -F FORWARD

# Ajouter rÃ¨gle NFQUEUE
sudo iptables -I FORWARD -s 10.10.1.10 -d 10.10.2.20 -j NFQUEUE --queue-num 0

# VÃ©rifier
sudo iptables -L FORWARD -n -v
```

### 3. Test Sequential
```bash
# Terminal 1 (filter node)
sudo ./build/tiger-fox --mode sequential --config config.json --queue-num 0

# Terminal 2 (injector node)
wrk -t 12 -c 400 -d 30s --latency http://10.10.2.20/
```

### 4. Test Parallel (3 workers)
```bash
# Terminal 1 (filter node) - Ctrl+C puis:
sudo ./build/tiger-fox --mode parallel --workers 3 --config config.json --queue-num 0

# Terminal 2 (injector node)
wrk -t 12 -c 400 -d 30s --latency http://10.10.2.20/
```

### 5. Variation du nombre de workers
```bash
for workers in 2 4 8; do
    echo "Testing with $workers workers"
    sudo ./build/tiger-fox --mode parallel --workers $workers --queue-num 0 &
    sleep 2
    # Sur injector: wrk -t 12 -c 400 -d 30s http://10.10.2.20/
    sudo pkill tiger-fox
    sleep 2
done
```

### 6. Nettoyer aprÃ¨s tests
```bash
sudo pkill tiger-fox
sudo iptables -D FORWARD -s 10.10.1.10 -d 10.10.2.20 -j NFQUEUE --queue-num 0
```

---

## ğŸ”¬ DÃ‰TAILS TECHNIQUES

### Fichiers modifiÃ©s/crÃ©Ã©s

#### Nouveaux fichiers (4)
1. `src/engine/fast_sequential_engine.h` - Engine sÃ©quentiel hash O(1)
2. `src/engine/fast_sequential_engine.cpp` - ImplÃ©mentation sequential
3. `src/engine/ultra_parallel_engine.h` - Engine parallel permanents workers
4. `src/engine/ultra_parallel_engine.cpp` - ImplÃ©mentation parallel

#### Fichiers modifiÃ©s (7)
1. `src/handlers/packet_handler.cpp` - SimplifiÃ© 722â†’295 lignes (-60%)
2. `src/handlers/packet_handler.h` - SupprimÃ© TCP reassembly
3. `src/tiger_system.cpp` - Mode selection + duplication rÃ¨gles
4. `src/tiger_system.h` - AjoutÃ© paramÃ¨tre mode
5. `src/main.cpp` - Parsing --mode argument
6. `CMakeLists.txt` - Liste sources mise Ã  jour
7. `rules/example_rules.json` - 23 rÃ¨gles L3/L4

#### Fichiers obsolÃ¨tes (retirÃ©s de CMake)
1. `src/engine/worker_pool.cpp` - RemplacÃ© par UltraParallelEngine
2. `src/handlers/tcp_reassembler.cpp` - SupprimÃ© (overhead inutile)

### Suppressions majeures

âœ… **TCP stream reassembly** - Overhead Ã©norme, inutile (HTTP tient en 1 paquet)  
âœ… **Worker pool avec mutex** - Contention, remplacÃ© par racing lock-free  
âœ… **Async verdict queue** - Buffering overhead, processing direct maintenant  
âœ… **Connection tracking** - ComplexitÃ© inutile pour L3/L4  
âœ… **Pending packets buffer** - SupprimÃ©, zero-copy maintenant  
âœ… **Statistics overhead** - RetirÃ© du hot path (debug mode uniquement)

### Optimisations clÃ©s

#### 1. Zero-copy stack allocation
```cpp
PacketData parsed_packet;  // Stack, pas malloc!
ParsePacket(data, len, parsed_packet);
engine->FilterPacket(parsed_packet);  // Read-only, pas de copie
```

#### 2. Inline synchronous processing
```cpp
// AVANT (async, queues, buffering):
HandlePacket â†’ AddToQueue â†’ Worker processes â†’ Verdict queue â†’ Callback
Temps: 3-5 mutex locks, 2-3 memory copies

// APRÃˆS (inline direct):
HandlePacket â†’ FilterPacket â†’ Verdict immÃ©diat
Temps: 0 mutex (sauf result write), 0 copies
```

#### 3. Lock-free racing
```cpp
// Pas de mutex pendant rule checking!
FilterResult result = engine->FilterPacket(packet);
if (result == DROP) {
    verdict_found.compare_exchange_strong(false, true);  // Atomic CAS
}
```

#### 4. Condition variables (vs thread spawn)
```cpp
// AVANT (par paquet):
for (worker : workers) {
    threads.push_back(std::thread(...));  // ~30Âµs overhead
}
for (thread : threads) thread.join();     // ~30Âµs overhead
// Total: ~60Âµs par paquet

// APRÃˆS (permanent):
packet_ready_cv.notify_all();  // ~200ns overhead
workers_done_cv.wait();        // ~50ns overhead
// Total: ~0.25Âµs par paquet

Gain: 60Âµs â†’ 0.25Âµs = 240x plus rapide!
```

---

## ğŸ“ˆ RÃ‰SULTATS ATTENDUS

### CritÃ¨res de succÃ¨s

#### Minimum viable
- âœ… Code compile sans erreurs
- âœ… Sequential mode > 2,000 req/s
- âœ… Parallel mode fonctionne sans crash

#### SuccÃ¨s complet
- âœ… Sequential mode > 2,500 req/s
- âœ… Parallel mode > Sequential mode
- âœ… Speed-up mesurable (> 1.5x)

#### Excellence
- âœ… Parallel > 4,000 req/s
- âœ… Speed-up > 2.5x
- âœ… Bat Suricata/Snort (si comparaison faite)

### MÃ©triques Ã  collecter

Pour chaque mode:
- **Throughput**: Requests/sec (wrk)
- **Latency**: avg, p50, p75, p90, p99 (wrk --latency)
- **CPU**: %usage (top/htop sur filter node)
- **Drops**: Nombre de paquets DROP vs ACCEPT
- **Speed-up**: Parallel req/s / Sequential req/s

### Graphiques attendus

```
Requests/sec vs Nombre de workers:
Sequential: ==================== (2,500 req/s)
Parallel 2: ============================ (3,500 req/s)
Parallel 3: ================================ (4,000 req/s)
Parallel 4: =================================== (4,200 req/s)
Parallel 8: ==================================== (4,300 req/s)
```

**Diminishing returns aprÃ¨s 4-6 workers** (synchronization overhead)

---

## âœ… VALIDATIONS

- [x] **Compilation**: Build successful (226K binary)
- [x] **Threads permanents**: CrÃ©Ã©s au dÃ©marrage, pas par paquet
- [x] **Partitionnement**: Chaque worker ~8 rÃ¨gles (vs 69 sequential)
- [x] **Synchronisation**: Condition variables ultra-rapides
- [x] **Racing**: Atomic CAS lock-free
- [x] **Zero-copy**: Packet partagÃ© via const pointer
- [x] **Hash O(1)**: IP/port lookups optimisÃ©s
- [x] **CompileIPRanges**: Masques rÃ©seau prÃ©-calculÃ©s
- [ ] **Benchmark**: Ã€ faire (attente tests CloudLab)

---

## ğŸ¯ CONCLUSION

**Architecture finale ultra-optimisÃ©e:**
- âœ… Mode Sequential: Hash O(1), 69 rÃ¨gles, baseline solide
- âœ… Mode Parallel: 3 workers permanents, 8 rÃ¨gles chacun, racing lock-free
- âœ… Partitionnement intelligent: Parallel plus rapide dans TOUS les cas
- âœ… Zero overhead: Pas de thread spawn, condition variables rapides
- âœ… Zero-copy: Stack allocation, const pointer sharing

**Speed-up attendu: 2.5-4x** ğŸš€

**PrÃªt pour benchmark CloudLab!** ğŸ“
